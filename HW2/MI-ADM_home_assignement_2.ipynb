{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI-ADM: home assignment 2\n",
    "\n",
    "  * **Deadline**: 15.05.2019 -2 points for late submission, the hard deadline is the first day of the exam period.\n",
    "  * **What to submit**: Just this notebook with your code and texts, not the dataset!\n",
    "  * **How to submit**: See the instructions at https://courses.fit.cvut.cz/MI-ADM/tutorials/index.html.\n",
    "  \n",
    "Generally speaking, the goal of this assignment is to apply **support vector machines on a classification problem**.\n",
    "\n",
    "What to do:\n",
    "  * Use the data from Spambase dataset http://archive.ics.uci.edu/ml/datasets/spambase.\n",
    "  * Train Support Vector Machine classification model directly (without any kernel approach) and evaluate its accuracy.\n",
    "  * Train Support Vector Machine classification model based on a kernel function (RBF, polynomial, etc.) of your choice and evaluate its accuracy.\n",
    "  * Compare the results with a random forest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('spambase.data', sep = ',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.nunique()\n",
    "cols = list(data.columns)\n",
    "cols[-1] = 'label'\n",
    "data.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drange(start, stop, step):\n",
    "    r = start\n",
    "    while r < stop:\n",
    "        yield r\n",
    "        r += step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing, I'll use cross-validation at ten folds.\n",
    "\n",
    "The test function was also created for testing one hyper-parameter and creating a method dependency graph on the values of this parameter. If not specified, check one value of random_state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of cores use to evaluate cross-validation\n",
    "n_jobs = -1\n",
    "# numbe of fold in cross-validation\n",
    "cv = 10\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "           'recall': 'recall',\n",
    "           'precision': 'precision'}\n",
    "# method for test model and iterate over one parameter (plot dependency), can be use only for evaluate one set of model.\n",
    "# default test parametr is for test with get parametrs for one time\n",
    "def test_model_and_hyper_parameter(model, data, s_params, label, check_results = [], \n",
    "                                  test_param = 'random_state', min_value_p = 42, max_value_p = 43, step_p = 1):\n",
    "    check = {}\n",
    "    for i in check_results:\n",
    "        check[i] = []\n",
    "    param_val = []\n",
    "    for i in drange(min_value_p, max_value_p, step_p):\n",
    "        s_params[test_param] = i\n",
    "        mod = model(**s_params)\n",
    "        scores = cross_validate(mod, data.drop(columns=[label]), data[label], \n",
    "                            scoring=scoring, cv=cv, n_jobs=n_jobs, return_train_score=True)\n",
    "        param_val.append(i)\n",
    "        for j in check_results:\n",
    "            check[j].append(np.sqrt(scores[j]).mean())\n",
    "    return param_val, check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all attributions are numeric, I have decided to standardize the data and compare the results on standardized data. I have tried both MinMax normalization and Standard normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martilad/MI-ADM2019/mi-adm-2018-martilad/__venv__/lib/python3.7/site-packages/sklearn/preprocessing/data.py:334: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "minMaxScaler = MinMaxScaler()\n",
    "dataMM = minMaxScaler.fit_transform(data.drop(columns=['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/martilad/MI-ADM2019/mi-adm-2018-martilad/__venv__/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/martilad/MI-ADM2019/mi-adm-2018-martilad/__venv__/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    }
   ],
   "source": [
    "standartScaler = StandardScaler()\n",
    "dataSS = standartScaler.fit_transform(data.drop(columns=['label']))\n",
    "# data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataMM = pd.DataFrame(dataMM)\n",
    "dataMM['label'] = data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSS = pd.DataFrame(dataSS)\n",
    "dataSS['label'] = data['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I try the Support vector machine (SVC) without any kernel approach. The linear kernel is set, because the rbs is default. The methods are tested on a non-standardized as well as on two normalized datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When specifying max_iter to -1, the stop is dependent on the termination condition, and it can be seen that the test ran, on an 8/16 processor, for a very long time (cross-validation gives the possibility run parallel evaluation).\n",
    "\n",
    "When I was limiting the number of iterations to obtain a model faster, non-standardized data losing accuracy. In the case of using the termination condition, this difference did not manifest, so training the model on non-standardized data takes much longer, but without restriction to maximum iteration, this model has very similar results to models with standardized data. As a result, Standard Scaling appears to be a suitable method for normalizing data for SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine with non normalized data: \n",
      "\t Train accuracy:  [0.9670340128388046]\n",
      "\t Test accuracy:  [0.9557163990334981]\n",
      "\n",
      "Support vector machine with data normalized with minMax normalization: \n",
      "\t Train accuracy:  [0.9513977475497883]\n",
      "\t Test accuracy:  [0.9474823932914556]\n",
      "\n",
      "Support vector machine with data normalized with standard normalization: \n",
      "\t Train accuracy:  [0.9663092294678988]\n",
      "\t Test accuracy:  [0.9590562249191124]\n",
      "CPU times: user 228 ms, sys: 96 ms, total: 324 ms\n",
      "Wall time: 9min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_iter = -1\n",
    "param, check = test_model_and_hyper_parameter(SVC, data, {'kernel': 'linear', 'max_iter': max_iter}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataMM, {'kernel': 'linear', 'max_iter': max_iter}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with minMax normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataSS, {'kernel': 'linear', 'max_iter': max_iter}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with standard normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with some kernel function (polynomial, rbf, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machinewith rbf kernel and with non normalized data: \n",
      "\t Train accuracy:  [0.9742872318023472]\n",
      "\t Test accuracy:  [0.902737850318023]\n",
      "\n",
      "Support vector machine with rbf kernel and with data normalized with minMax normalization: \n",
      "\t Train accuracy:  [0.9003657854301788]\n",
      "\t Test accuracy:  [0.8963832641186963]\n",
      "\n",
      "Support vector machine with rbf kernel and with data normalized with standard normalization: \n",
      "\t Train accuracy:  [0.9736427091928871]\n",
      "\t Test accuracy:  [0.9641463592914246]\n",
      "CPU times: user 152 ms, sys: 8 ms, total: 160 ms\n",
      "Wall time: 9.96 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_iter = -1\n",
    "param, check = test_model_and_hyper_parameter(SVC, data, {'kernel': 'rbf', 'max_iter': max_iter}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machinewith rbf kernel and with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataMM, {'kernel': 'rbf', 'max_iter': max_iter}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with rbf kernel and with data normalized with minMax normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataSS, {'kernel': 'rbf', 'max_iter': max_iter}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with rbf kernel and with data normalized with standard normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first kernel I try is the RBF kernel. I play with set gamma, but I could not achieve a better result than the default value. The results show that this method works well for standardized data using StandartScaller. With this model and normalized data, I also managed to achieve the highest accuracy among all models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine with non normalized data: \n",
      "\t Train accuracy:  [0.959512221941959]\n",
      "\t Test accuracy:  [0.9515544646503166]\n",
      "\n",
      "Support vector machine with data normalized with minMax normalization: \n",
      "\t Train accuracy:  [0.8348372289857828]\n",
      "\t Test accuracy:  [0.8345743568349497]\n",
      "\n",
      "Support vector machine with data normalized with standard normalization: \n",
      "\t Train accuracy:  [0.9626530971111837]\n",
      "\t Test accuracy:  [0.9584073874414466]\n",
      "CPU times: user 168 ms, sys: 4 ms, total: 172 ms\n",
      "Wall time: 16.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_iter = -1\n",
    "param, check = test_model_and_hyper_parameter(SVC, data, {'kernel': 'poly', 'max_iter': max_iter, 'degree': 1}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataMM, {'kernel': 'poly', 'max_iter': max_iter, 'degree': 1}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with minMax normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataSS, {'kernel': 'poly', 'max_iter': max_iter, 'degree': 1}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with standard normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine with non normalized data: \n",
      "\t Train accuracy:  [0.7174013292427945]\n",
      "\t Test accuracy:  [0.7109888481696478]\n",
      "\n",
      "Support vector machine with data normalized with minMax normalization: \n",
      "\t Train accuracy:  [0.7784312621214857]\n",
      "\t Test accuracy:  [0.7784314013688217]\n",
      "\n",
      "Support vector machine with data normalized with standard normalization: \n",
      "\t Train accuracy:  [0.9301974208900216]\n",
      "\t Test accuracy:  [0.9159871628905918]\n",
      "CPU times: user 156 ms, sys: 12 ms, total: 168 ms\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_iter = 30000000\n",
    "param, check = test_model_and_hyper_parameter(SVC, data, {'kernel': 'poly', 'max_iter': max_iter, 'degree': 2}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataMM, {'kernel': 'poly', 'max_iter': max_iter, 'degree': 2}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with minMax normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataSS, {'kernel': 'poly', 'max_iter': max_iter, 'degree': 2}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with standard normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a second kernel, I tried polynomial with one and two degrees. With a higher degree, the success of the model only deteriorated. In the first stage, that is, the line function, it can be seen that it corresponds to the first test, i.e., the linear kernel. Only for minMax normalization are the results significantly worse.\n",
    "\n",
    "For polynomial with degree 2, I had to limit the number of iterations to make it computable. So with the quadratic polynomial kernel function failed to reach the previous results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support vector machine with non normalized data: \n",
      "\t Train accuracy:  [0.5917488853020746]\n",
      "\t Test accuracy:  [0.589977662821878]\n",
      "\n",
      "Support vector machine with data normalized with minMax normalization: \n",
      "\t Train accuracy:  [0.8348230246655438]\n",
      "\t Test accuracy:  [0.834445141005372]\n",
      "\n",
      "Support vector machine with data normalized with standard normalization: \n",
      "\t Train accuracy:  [0.937830719353542]\n",
      "\t Test accuracy:  [0.9325673063684551]\n",
      "CPU times: user 148 ms, sys: 12 ms, total: 160 ms\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_iter = -1\n",
    "param, check = test_model_and_hyper_parameter(SVC, data, {'kernel': 'sigmoid'}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataMM, {'kernel': 'sigmoid'}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with minMax normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "param, check = test_model_and_hyper_parameter(SVC, dataSS, {'kernel': 'sigmoid'}, 'label',  \n",
    "                                          ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall', 'test_precision', 'train_precision' ])\n",
    "print('Support vector machine with data normalized with standard normalization: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last I tried sigmoid kernel, but it wasn't successful. But I found that this core function is very sensitive to data standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare model to random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest with non normalized data: \n",
      "\t Train accuracy:  [0.9622649182465489]\n",
      "\t Test accuracy:  [0.9575533092443974]\n",
      "\n",
      "Random forest machine with non normalized data: \n",
      "\t Train accuracy:  [0.9622774350558112]\n",
      "\t Test accuracy:  [0.9575533092443974]\n",
      "\n",
      "Random forest machine with non normalized data: \n",
      "\t Train accuracy:  [0.9622649182465489]\n",
      "\t Test accuracy:  [0.9575533092443974]\n",
      "\n",
      "CPU times: user 148 ms, sys: 4 ms, total: 152 ms\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param, check = test_model_and_hyper_parameter(RandomForestClassifier, data, {'max_depth': 4, 'random_state':1, \n",
    "                                        'min_samples_split':2, 'min_samples_leaf':1, 'n_estimators':90}, 'label', \n",
    "                                        ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall','test_precision', 'train_precision' ] )\n",
    "print('Random forest with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "\n",
    "param, check = test_model_and_hyper_parameter(RandomForestClassifier, dataMM, {'max_depth': 4, 'random_state':1, \n",
    "                                        'min_samples_split':2, 'min_samples_leaf':1, 'n_estimators':90}, 'label', \n",
    "                                       ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall','test_precision', 'train_precision' ] )\n",
    "print('Random forest machine with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()\n",
    "\n",
    "param, check = test_model_and_hyper_parameter(RandomForestClassifier, dataSS, {'max_depth': 4, 'random_state':1, \n",
    "                                        'min_samples_split':2, 'min_samples_leaf':1, 'n_estimators':90}, 'label', \n",
    "                                        ['score_time','fit_time', 'test_accuracy', 'train_accuracy', \n",
    "                                          'test_recall', 'train_recall','test_precision', 'train_precision' ] )\n",
    "print('Random forest machine with non normalized data: ')\n",
    "print(\"\\t Train accuracy: \", check['train_accuracy'])\n",
    "print(\"\\t Test accuracy: \", check['test_accuracy'])\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare to decision trees, I did not expect any sensitivity to attribute standardization, but I gave an example and measured standardized datasets. Thus, compared to the support vector machine, the decision-making forest is insensitive to standardization. The SVC is sensitive, which I assumed. The more complex the kernel function is, the more critical it is to normalize the data for good results from SVC.\n",
    "\n",
    "With the support vector machine, I have achieved the best result with kernel RBF -> 0.964 test accuracy.\n",
    "And 0.9575 for Random Decision Forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
